/
// COMPLETE iOS VOICE ASSISTANT APP PROJECT
// Ready for deployment to App Store
//

// ==========================================
// 1. PROJECT STRUCTURE
// ==========================================
/*
VoiceAssistantApp/
‚îú‚îÄ‚îÄ VoiceAssistantApp/
‚îÇ   ‚îú‚îÄ‚îÄ AppDelegate.swift
‚îÇ   ‚îú‚îÄ‚îÄ SceneDelegate.swift
‚îÇ   ‚îú‚îÄ‚îÄ ViewController.swift
‚îÇ   ‚îú‚îÄ‚îÄ Info.plist
‚îÇ   ‚îú‚îÄ‚îÄ Assets.xcassets/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ AppIcon.appiconset/
‚îÇ   ‚îú‚îÄ‚îÄ LaunchScreen.storyboard
‚îÇ   ‚îú‚îÄ‚îÄ Main.storyboard
‚îÇ   ‚îî‚îÄ‚îÄ voice_assistant.html
‚îú‚îÄ‚îÄ VoiceAssistantApp.xcodeproj
‚îî‚îÄ‚îÄ VoiceAssistantAppTests/
*/

// ==========================================
// 2. AppDelegate.swift
// ==========================================

import UIKit

@main
class AppDelegate: UIResponder, UIApplicationDelegate {

    func application(_ application: UIApplication, didFinishLaunchingWithOptions launchOptions: [UIApplication.LaunchOptionsKey: Any]?) -> Bool {
        // Configure app appearance
        setupAppearance()
        
        // Request permissions on app launch
        requestPermissions()
        
        return true
    }
    
    private func setupAppearance() {
        // Set status bar style
        UIApplication.shared.statusBarStyle = .lightContent
        
        // Configure navigation bar appearance
        let appearance = UINavigationBarAppearance()
        appearance.configureWithOpaqueBackground()
        appearance.backgroundColor = UIColor.systemBlue
        appearance.titleTextAttributes = [.foregroundColor: UIColor.white]
        
        UINavigationBar.appearance().standardAppearance = appearance
        UINavigationBar.appearance().scrollEdgeAppearance = appearance
    }
    
    private func requestPermissions() {
        // Microphone permission
        AVAudioSession.sharedInstance().requestRecordPermission { granted in
            DispatchQueue.main.async {
                if granted {
                    print("‚úÖ Microphone permission granted")
                } else {
                    print("‚ùå Microphone permission denied")
                }
            }
        }
        
        // Speech recognition permission
        SFSpeechRecognizer.requestAuthorization { authStatus in
            DispatchQueue.main.async {
                switch authStatus {
                case .authorized:
                    print("‚úÖ Speech recognition authorized")
                case .denied, .restricted, .notDetermined:
                    print("‚ùå Speech recognition not authorized")
                @unknown default:
                    print("‚ùå Unknown speech recognition status")
                }
            }
        }
    }

    // MARK: UISceneSession Lifecycle
    func application(_ application: UIApplication, configurationForConnecting connectingSceneSession: UISceneSession, options: UIScene.ConnectionOptions) -> UISceneConfiguration {
        return UISceneConfiguration(name: "Default Configuration", sessionRole: connectingSceneSession.role)
    }

    func application(_ application: UIApplication, didDiscardSceneSessions sceneSessions: Set<UISceneSession>) {
    }
}

// ==========================================
// 3. SceneDelegate.swift
// ==========================================

import UIKit

class SceneDelegate: UIResponder, UIWindowSceneDelegate {

    var window: UIWindow?

    func scene(_ scene: UIScene, willConnectTo session: UISceneSession, options connectionOptions: UIScene.ConnectionOptions) {
        guard let windowScene = (scene as? UIWindowScene) else { return }
        
        window = UIWindow(windowScene: windowScene)
        
        // Create the main view controller
        let mainViewController = ViewController()
        let navigationController = UINavigationController(rootViewController: mainViewController)
        
        // Configure navigation controller
        navigationController.navigationBar.prefersLargeTitles = true
        navigationController.navigationBar.barTintColor = UIColor.systemBlue
        navigationController.navigationBar.tintColor = UIColor.white
        
        window?.rootViewController = navigationController
        window?.makeKeyAndVisible()
    }

    func sceneDidDisconnect(_ scene: UIScene) {}
    func sceneDidBecomeActive(_ scene: UIScene) {}
    func sceneWillResignActive(_ scene: UIScene) {}
    func sceneWillEnterForeground(_ scene: UIScene) {}
    func sceneDidEnterBackground(_ scene: UIScene) {}
}

// ==========================================
// 4. ViewController.swift (Main App Logic)
// ==========================================

import UIKit
import WebKit
import Speech
import AVFoundation

class ViewController: UIViewController, WKNavigationDelegate, WKUIDelegate, WKScriptMessageHandler {
    
    // MARK: - Properties
    private var webView: WKWebView!
    private var activityIndicator: UIActivityIndicatorView!
    private var errorView: UIView!
    private var retryButton: UIButton!
    
    // MARK: - Lifecycle
    override func viewDidLoad() {
        super.viewDidLoad()
        setupUI()
        setupWebView()
        loadVoiceAssistant()
    }
    
    override func viewDidAppear(_ animated: Bool) {
        super.viewDidAppear(animated)
        checkPermissions()
    }
    
    // MARK: - Setup Methods
    private func setupUI() {
        title = "AI Voice Assistant"
        view.backgroundColor = UIColor.systemBackground
        
        // Setup activity indicator
        activityIndicator = UIActivityIndicatorView(style: .large)
        activityIndicator.translatesAutoresizingMaskIntoConstraints = false
        activityIndicator.color = UIColor.systemBlue
        view.addSubview(activityIndicator)
        
        // Setup error view
        setupErrorView()
        
        // Constraints
        NSLayoutConstraint.activate([
            activityIndicator.centerXAnchor.constraint(equalTo: view.centerXAnchor),
            activityIndicator.centerYAnchor.constraint(equalTo: view.centerYAnchor)
        ])
    }
    
    private func setupErrorView() {
        errorView = UIView()
        errorView.translatesAutoresizingMaskIntoConstraints = false
        errorView.backgroundColor = UIColor.systemBackground
        errorView.isHidden = true
        view.addSubview(errorView)
        
        let errorLabel = UILabel()
        errorLabel.text = "‚ùå Failed to load Voice Assistant"
        errorLabel.textAlignment = .center
        errorLabel.font = UIFont.systemFont(ofSize: 18, weight: .medium)
        errorLabel.textColor = UIColor.systemRed
        errorLabel.numberOfLines = 0
        errorLabel.translatesAutoresizingMaskIntoConstraints = false
        
        retryButton = UIButton(type: .system)
        retryButton.setTitle("üîÑ Retry", for: .normal)
        retryButton.titleLabel?.font = UIFont.systemFont(ofSize: 16, weight: .semibold)
        retryButton.backgroundColor = UIColor.systemBlue
        retryButton.setTitleColor(.white, for: .normal)
        retryButton.layer.cornerRadius = 25
        retryButton.translatesAutoresizingMaskIntoConstraints = false
        retryButton.addTarget(self, action: #selector(retryButtonTapped), for: .touchUpInside)
        
        errorView.addSubview(errorLabel)
        errorView.addSubview(retryButton)
        
        NSLayoutConstraint.activate([
            errorView.topAnchor.constraint(equalTo: view.safeAreaLayoutGuide.topAnchor),
            errorView.leadingAnchor.constraint(equalTo: view.leadingAnchor),
            errorView.trailingAnchor.constraint(equalTo: view.trailingAnchor),
            errorView.bottomAnchor.constraint(equalTo: view.bottomAnchor),
            
            errorLabel.centerXAnchor.constraint(equalTo: errorView.centerXAnchor),
            errorLabel.centerYAnchor.constraint(equalTo: errorView.centerYAnchor, constant: -30),
            errorLabel.leadingAnchor.constraint(equalTo: errorView.leadingAnchor, constant: 20),
            errorLabel.trailingAnchor.constraint(equalTo: errorView.trailingAnchor, constant: -20),
            
            retryButton.topAnchor.constraint(equalTo: errorLabel.bottomAnchor, constant: 20),
            retryButton.centerXAnchor.constraint(equalTo: errorView.centerXAnchor),
            retryButton.widthAnchor.constraint(equalToConstant: 120),
            retryButton.heightAnchor.constraint(equalToConstant: 50)
        ])
    }
    
    private func setupWebView() {
        let configuration = WKWebViewConfiguration()
        configuration.allowsInlineMediaPlayback = true
        configuration.mediaTypesRequiringUserActionForPlayback = []
        
        // Add script message handler for communication between web and native
        configuration.userContentController.add(self, name: "voiceAssistant")
        
        // Allow microphone access
        configuration.preferences.setValue(true, forKey: "allowFileAccessFromFileURLs")
        configuration.setValue(true, forKey: "allowUniversalAccessFromFileURLs")
        
        webView = WKWebView(frame: .zero, configuration: configuration)
        webView.translatesAutoresizingMaskIntoConstraints = false
        webView.navigationDelegate = self
        webView.uiDelegate = self
        webView.backgroundColor = UIColor.systemBackground
        webView.scrollView.backgroundColor = UIColor.systemBackground
        
        view.addSubview(webView)
        view.sendSubviewToBack(webView)
        
        NSLayoutConstraint.activate([
            webView.topAnchor.constraint(equalTo: view.safeAreaLayoutGuide.topAnchor),
            webView.leadingAnchor.constraint(equalTo: view.leadingAnchor),
            webView.trailingAnchor.constraint(equalTo: view.trailingAnchor),
            webView.bottomAnchor.constraint(equalTo: view.bottomAnchor)
        ])
    }
    
    private func loadVoiceAssistant() {
        activityIndicator.startAnimating()
        
        guard let htmlPath = Bundle.main.path(forResource: "voice_assistant", ofType: "html") else {
            showError("Voice Assistant HTML file not found")
            return
        }
        
        let htmlURL = URL(fileURLWithPath: htmlPath)
        webView.loadFileURL(htmlURL, allowingReadAccessTo: htmlURL.deletingLastPathComponent())
    }
    
    private func checkPermissions() {
        let microphoneStatus = AVAudioSession.sharedInstance().recordPermission
        let speechStatus = SFSpeechRecognizer.authorizationStatus()
        
        if microphoneStatus != .granted || speechStatus != .authorized {
            showPermissionAlert()
        }
    }
    
    private func showPermissionAlert() {
        let alert = UIAlertController(
            title: "Permissions Required",
            message: "This app needs microphone and speech recognition permissions to work properly.",
            preferredStyle: .alert
        )
        
        alert.addAction(UIAlertAction(title: "Settings", style: .default) { _ in
            if let settingsURL = URL(string: UIApplication.openSettingsURLString) {
                UIApplication.shared.open(settingsURL)
            }
        })
        
        alert.addAction(UIAlertAction(title: "Cancel", style: .cancel))
        
        present(alert, animated: true)
    }
    
    private func showError(_ message: String) {
        activityIndicator.stopAnimating()
        errorView.isHidden = false
        print("‚ùå Error: \(message)")
    }
    
    @objc private func retryButtonTapped() {
        errorView.isHidden = true
        loadVoiceAssistant()
    }
    
    // MARK: - WKNavigationDelegate
    func webView(_ webView: WKWebView, didFinish navigation: WKNavigation!) {
        activityIndicator.stopAnimating()
        print("‚úÖ Voice Assistant loaded successfully")
    }
    
    func webView(_ webView: WKWebView, didFail navigation: WKNavigation!, withError error: Error) {
        showError("Failed to load: \(error.localizedDescription)")
    }
    
    func webView(_ webView: WKWebView, didFailProvisionalNavigation navigation: WKNavigation!, withError error: Error) {
        showError("Failed to load: \(error.localizedDescription)")
    }
    
    // MARK: - WKUIDelegate
    func webView(_ webView: WKWebView, runJavaScriptAlertPanelWithMessage message: String, initiatedByFrame frame: WKFrameInfo, completionHandler: @escaping () -> Void) {
        let alert = UIAlertController(title: "Voice Assistant", message: message, preferredStyle: .alert)
        alert.addAction(UIAlertAction(title: "OK", style: .default) { _ in completionHandler() })
        present(alert, animated: true)
    }
    
    func webView(_ webView: WKWebView, runJavaScriptConfirmPanelWithMessage message: String, initiatedByFrame frame: WKFrameInfo, completionHandler: @escaping (Bool) -> Void) {
        let alert = UIAlertController(title: "Voice Assistant", message: message, preferredStyle: .alert)
        alert.addAction(UIAlertAction(title: "OK", style: .default) { _ in completionHandler(true) })
        alert.addAction(UIAlertAction(title: "Cancel", style: .cancel) { _ in completionHandler(false) })
        present(alert, animated: true)
    }
    
    // MARK: - WKScriptMessageHandler
    func userContentController(_ userContentController: WKUserContentController, didReceive message: WKScriptMessage) {
        if message.name == "voiceAssistant" {
            if let body = message.body as? [String: Any] {
                handleWebMessage(body)
            }
        }
    }
    
    private func handleWebMessage(_ message: [String: Any]) {
        guard let action = message["action"] as? String else { return }
        
        switch action {
        case "permissionRequest":
            checkPermissions()
        case "error":
            if let errorMessage = message["message"] as? String {
                print("üî¥ Web Error: \(errorMessage)")
            }
        case "success":
            if let successMessage = message["message"] as? String {
                print("‚úÖ Web Success: \(successMessage)")
            }
        default:
            break
        }
    }
}

// ==========================================
// 5. Info.plist Configuration
// ==========================================
/*
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
    <!-- Basic App Configuration -->
    <key>CFBundleDevelopmentRegion</key>
    <string>$(DEVELOPMENT_LANGUAGE)</string>
    <key>CFBundleDisplayName</key>
    <string>AI Voice Assistant</string>
    <key>CFBundleExecutable</key>
    <string>$(EXECUTABLE_NAME)</string>
    <key>CFBundleIdentifier</key>
    <string>$(PRODUCT_BUNDLE_IDENTIFIER)</string>
    <key>CFBundleInfoDictionaryVersion</key>
    <string>6.0</string>
    <key>CFBundleName</key>
    <string>$(PRODUCT_NAME)</string>
    <key>CFBundlePackageType</key>
    <string>$(PRODUCT_BUNDLE_PACKAGE_TYPE)</string>
    <key>CFBundleShortVersionString</key>
    <string>1.0</string>
    <key>CFBundleVersion</key>
    <string>1</string>
    <key>LSRequiresIPhoneOS</key>
    <true/>
    
    <!-- üé§ CRITICAL PERMISSIONS -->
    <key>NSMicrophoneUsageDescription</key>
    <string>This app needs microphone access to listen to your voice commands and provide AI-powered responses.</string>
    
    <key>NSSpeechRecognitionUsageDescription</key>
    <string>This app uses speech recognition to understand your voice commands and convert them to text for AI processing.</string>
    
    <!-- üåê NETWORK SECURITY -->
    <key>NSAppTransportSecurity</key>
    <dict>
        <key>NSExceptionDomains</key>
        <dict>
            <key>api.openai.com</key>
            <dict>
                <key>NSExceptionRequiresForwardSecrecy</key>
                <false/>
                <key>NSExceptionMinimumTLSVersion</key>
                <string>TLSv1.2</string>
                <key>NSThirdPartyExceptionRequiresForwardSecrecy</key>
                <false/>
            </dict>
        </dict>
    </dict>
    
    <!-- üì± DEVICE CAPABILITIES -->
    <key>UIRequiredDeviceCapabilities</key>
    <array>
        <string>microphone</string>
    </array>
    
    <!-- üîÑ INTERFACE ORIENTATIONS -->
    <key>UISupportedInterfaceOrientations</key>
    <array>
        <string>UIInterfaceOrientationPortrait</string>
        <string>UIInterfaceOrientationLandscapeLeft</string>
        <string>UIInterfaceOrientationLandscapeRight</string>
    </array>
    
    <key>UISupportedInterfaceOrientations~ipad</key>
    <array>
        <string>UIInterfaceOrientationPortrait</string>
        <string>UIInterfaceOrientationPortraitUpsideDown</string>
        <string>UIInterfaceOrientationLandscapeLeft</string>
        <string>UIInterfaceOrientationLandscapeRight</string>
    </array>
    
    <!-- üì∫ SCENE CONFIGURATION -->
    <key>UIApplicationSceneManifest</key>
    <dict>
        <key>UIApplicationSupportsMultipleScenes</key>
        <false/>
        <key>UISceneConfigurations</key>
        <dict>
            <key>UIWindowSceneSessionRoleApplication</key>
            <array>
                <dict>
                    <key>UISceneConfigurationName</key>
                    <string>Default Configuration</string>
                    <key>UISceneDelegateClassName</key>
                    <string>$(PRODUCT_MODULE_NAME).SceneDelegate</string>
                </dict>
            </array>
        </dict>
    </dict>
    
    <!-- üöÄ LAUNCH CONFIGURATION -->
    <key>UILaunchStoryboardName</key>
    <string>LaunchScreen</string>
    
    <!-- üìä STATUS BAR -->
    <key>UIStatusBarStyle</key>
    <string>UIStatusBarStyleLightContent</string>
    <key>UIViewControllerBasedStatusBarAppearance</key>
    <false/>
    
    <!-- üé® APP APPEARANCE -->
    <key>UIUserInterfaceStyle</key>
    <string>Automatic</string>
</dict>
</plist>
*/

// ==========================================
// 6. HTML FILE CONTENT (voice_assistant.html)
// ==========================================
/*
Save this as voice_assistant.html in your iOS project bundle:

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="theme-color" content="#007AFF">
    <title>AI Voice Assistant</title>
    
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: linear-gradient(135deg, #007AFF 0%, #5856D6 100%);
            min-height: 100vh;
            display: flex;
            flex-direction: column;
            touch-action: manipulation;
            padding-top: env(safe-area-inset-top, 0);
            padding-bottom: env(safe-area-inset-bottom, 0);
        }

        .app-container {
            flex: 1;
            display: flex;
            flex-direction: column;
            padding: 20px;
            max-width: 100%;
        }

        .header {
            text-align: center;
            color: white;
            margin-bottom: 30px;
        }

        .header h1 {
            font-size: 28px;
            font-weight: 700;
            margin-bottom: 8px;
            text-shadow: 0 2px 4px rgba(0,0,0,0.3);
        }

        .status-card {
            background: rgba(255, 255, 255, 0.95);
            backdrop-filter: blur(10px);
            border-radius: 20px;
            padding: 25px;
            margin-bottom: 25px;
            box-shadow: 0 8px 32px rgba(0,0,0,0.1);
            text-align: center;
        }

        .status-text {
            font-size: 18px;
            font-weight: 600;
            color: #333;
        }

        .microphone-container {
            display: flex;
            justify-content: center;
            align-items: center;
            margin: 30px 0;
        }

        .microphone {
            width: 120px;
            height: 120px;
            border-radius: 50%;
            background: linear-gradient(135deg, #ff6b6b, #ee5a52);
            display: flex;
            justify-content: center;
            align-items: center;
            font-size: 48px;
            color: white;
            cursor: pointer;
            transition: all 0.3s ease;
            box-shadow: 0 8px 24px rgba(238, 90, 82, 0.4);
        }

        .microphone.listening {
            background: linear-gradient(135deg, #4CAF50, #45a049);
            animation: pulse 1.5s infinite;
        }

        .microphone.processing {
            background: linear-gradient(135deg, #ff9800, #f57c00);
            animation: spin 2s linear infinite;
        }

        @keyframes pulse {
            0% { transform: scale(1); }
            50% { transform: scale(1.1); }
            100% { transform: scale(1); }
        }

        @keyframes spin {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }

        .controls {
            display: flex;
            gap: 15px;
            margin-bottom: 25px;
        }

        .btn {
            flex: 1;
            padding: 16px 24px;
            border: none;
            border-radius: 25px;
            font-size: 16px;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.3s ease;
            text-transform: uppercase;
            letter-spacing: 0.5px;
        }

        .btn-primary {
            background: linear-gradient(135deg, #007AFF, #5856D6);
            color: white;
            box-shadow: 0 4px 15px rgba(0, 122, 255, 0.3);
        }

        .btn-danger {
            background: linear-gradient(135deg, #FF3B30, #D70015);
            color: white;
            box-shadow: 0 4px 15px rgba(255, 59, 48, 0.3);
        }

        .btn:disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }

        .response-container {
            flex: 1;
            background: rgba(255, 255, 255, 0.95);
            backdrop-filter: blur(10px);
            border-radius: 20px;
            padding: 25px;
            box-shadow: 0 8px 32px rgba(0,0,0,0.1);
            overflow-y: auto;
            min-height: 200px;
            position: relative;
        }

        .response-text {
            font-size: 16px;
            line-height: 1.6;
            color: #333;
        }

        .placeholder-text {
            color: #999;
            font-style: italic;
            text-align: center;
            margin-top: 50px;
        }

        .error-message {
            background: #ffebee;
            color: #c62828;
            padding: 15px;
            border-radius: 10px;
            margin: 10px 0;
            border-left: 4px solid #f44336;
        }

        .clear-btn {
            position: absolute;
            top: 15px;
            right: 15px;
            background: #ff9800;
            color: white;
            border: none;
            border-radius: 20px;
            width: 40px;
            height: 40px;
            font-size: 18px;
            cursor: pointer;
        }

        .loading {
            display: flex;
            justify-content: center;
            align-items: center;
            gap: 5px;
            margin: 20px 0;
        }

        .loading-dot {
            width: 10px;
            height: 10px;
            border-radius: 50%;
            background: #007AFF;
            animation: loading 1.4s infinite both;
        }

        .loading-dot:nth-child(1) { animation-delay: -0.32s; }
        .loading-dot:nth-child(2) { animation-delay: -0.16s; }

        @keyframes loading {
            0%, 80%, 100% { transform: scale(0.8); opacity: 0.5; }
            40% { transform: scale(1.2); opacity: 1; }
        }
    </style>
</head>
<body>
    <div class="app-container">
        <div class="header">
            <h1>ü§ñ AI Voice Assistant</h1>
        </div>

        <div class="status-card">
            <div class="status-text" id="statusText">Ready to listen</div>
        </div>

        <div class="microphone-container">
            <div class="microphone" id="microphoneBtn">üé§</div>
        </div>

        <div class="controls">
            <button class="btn btn-primary" id="startBtn">Start Listening</button>
            <button class="btn btn-danger" id="stopBtn" disabled>Stop Listening</button>
        </div>

        <div class="response-container">
            <button class="clear-btn" id="clearBtn" title="Clear">üóëÔ∏è</button>
            <div class="response-text" id="responseText">
                <div class="placeholder-text">
                    üéØ AI responses will appear here...<br><br>
                    üí° Try asking: "What's the weather like?" or "Tell me a joke!"
                </div>
            </div>
        </div>
    </div>

    <script>
        class VoiceAssistant {
            constructor() {
                this.statusText = document.getElementById('statusText');
                this.responseText = document.getElementById('responseText');
                this.microphoneBtn = document.getElementById('microphoneBtn');
                this.startBtn = document.getElementById('startBtn');
                this.stopBtn = document.getElementById('stopBtn');
                this.clearBtn = document.getElementById('clearBtn');

                this.recognition = null;
                this.synthesis = window.speechSynthesis;
                this.isListening = false;
                this.isProcessing = false;

                // OpenAI Configuration - REPLACE WITH YOUR API KEY
                this.openAIKey = 'sk-your-openai-api-key-here'; // IMPORTANT: Replace this!
                this.openAIURL = 'https://api.openai.com/v1/chat/completions';

                this.init();
            }

            init() {
                this.setupSpeechRecognition();
                this.setupEventListeners();
                this.updateUI();
                this.sendMessageToNative('success', 'Voice Assistant initialized');
            }

            sendMessageToNative(action, message) {
                if (window.webkit && window.webkit.messageHandlers && window.webkit.messageHandlers.voiceAssistant) {
                    window.webkit.messageHandlers.voiceAssistant.postMessage({
                        action: action,
                        message: message
                    });
                }
            }

            setupSpeechRecognition() {
                if ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {
                    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
                    this.recognition = new SpeechRecognition();
                    
                    this.recognition.continuous = false;
                    this.recognition.interimResults = true;
                    this.recognition.lang = 'en-US';

                    this.recognition.onstart = () => {
                        this.isListening = true;
                        this.updateStatus('üé§ Listening... Speak now!');
                        this.updateUI();
                    };

                    this.recognition.onresult = (event) => {
                        let finalTranscript = '';
                        let interimTranscript = '';

                        for (let i = event.resultIndex; i < event.results.length; i++) {
                            const transcript = event.results[i][0].transcript;
                            if (event.results[i].isFinal) {
                                finalTranscript += transcript;
                            } else {
                                interimTranscript += transcript;
                            }
                        }

                        if (interimTranscript) {
                            this.updateStatus(`üé§ Listening: "${interimTranscript}"`);
                        }

                        if (finalTranscript) {
                            this.processUserInput(finalTranscript.trim());
                        }
                    };

                    this.recognition.onerror = (event) => {
                        console.error('Speech recognition error:', event.error);
                        this.showError(`Speech recognition error: ${event.error}`);
                        this.sendMessageToNative('error', `Speech recognition error: ${event.error}`);
                        this.stopListening();
                    };

                    this.recognition.onend = () => {
                        if (this.isListening && !this.isProcessing) {
                            this.stopListening();
                        }
                    };
                } else {
                    this.showError('Speech recognition not supported');
                    this.sendMessageToNative('error', 'Speech recognition not supported');
                }
            }

            setupEventListeners() {
                this.startBtn.addEventListener('click', () => this.startListening());
                this.stopBtn.addEventListener('click', () => this.stopListening());
                this.clearBtn.addEventListener('click', () => this.clearResponse());
                this.microphoneBtn.addEventListener('click', () => {
                    if (this.isListening) {
                        this.stopListening();
                    } else {
                        this.startListening();
                    }
                });
            }

            startListening() {
                if (!this.recognition) {
                    this.showError('Speech recognition not available');
                    this.sendMessageToNative('permissionRequest', 'Speech recognition permission needed');
                    return;
                }

                if (this.isListening) {
                    return;
                }

                try {
                    this.recognition.start();
                } catch (error) {
                    console.error('Error starting recognition:', error);
                    this.showError('Failed to start listening');
                    this.sendMessageToNative('error', 'Failed to start listening');
                }
            }

            stopListening() {
                if (this.recognition && this.isListening) {
                    this.recognition.stop();
                }
                this.isListening = false;
                this.updateStatus('Ready to listen');
                this.updateUI();
            }

            async processUserInput(input) {
                this.isProcessing = true;
                this.isListening = false;
                this.updateStatus('ü§ñ Processing your request...');
                this.updateUI();

                this.showLoading();

                try {
                    const response = await this.sendToChatGPT(input);
                    this.displayResponse(response);
                    this.speakResponse(response);
                    this.sendMessageToNative('success', 'Response generated successfully');
                } catch (error) {
                    console.error('Error processing input:', error);
                    this.showError('Failed to get AI response. Please try again.');
                    this.sendMessageToNative('error', error.message);
                } finally {
                    this.isProcessing = false;
                    this.updateStatus('Ready to listen');
                    this.updateUI();
                }
            }

            async sendToChatGPT(message) {
                if (this.openAIKey === 'sk-your-openai-api-key-here' || !this.openAIKey) {
                    throw new Error('Please set your OpenAI API key in the code');
                }

                const response = await fetch(this.openAIURL, {
                    method: 'POST',
                    headers: {
                        'Authorization': `Bearer ${this.openAIKey}`,
                        'Content-Type': 'application/json',
                    },
                    body: JSON.stringify({
                        model: 'gpt-3.5-turbo',
                        messages: [
                            {
                                role: 'system',
                                content: 'You are a helpful voice assistant. Provide concise, friendly responses suitable for speech. Keep responses under 100 words.'
                            },
                            {
                                role: 'user',
                                content: message
                            }
                        ],
                        max_tokens: 150,
                        temperature: 0.7
                    })
                });

                if (!response.ok) {
                    const errorData = await response.json().catch(() => ({}));
                    throw new Error(errorData.error?.message || `HTTP ${response.status}`);
                }

                const data = await response.json();
                
                if (!data.choices || !data.choices[0] || !data.choices[0].message) {
                    throw new Error('Invalid response format from OpenAI');
                }

                return data.choices[0].message.content.trim();
            }

            displayResponse(response) {
                this.responseText.innerHTML = `
                    <div style="margin-bottom: 15px;">
                        <strong style="color: #007AFF;">ü§ñ AI Assistant:</strong>
                    </div>
                    <div style="font-size: 16px; line-height: 1.6;">
                        ${response}
                    </div>
                    <div style="margin-top: 15px; font-size: 12px; color: #666;">
                        ${new Date().toLocaleTimeString()}
                    </div>
                `;
            }

            speakResponse(text) {
                this.synthesis.cancel();
                this.updateStatus('üó£Ô∏è Speaking response...');

                const utterance = new SpeechSynthesisUtterance(text);
                utterance.rate = 0.9;
                utterance.pitch = 1;
                utterance.volume = 1;

                utterance.onend = () => {
                    this.updateStatus('‚úÖ Ready to listen');
                };

                utterance.onerror = (event) => {
                    console.error('Speech synthesis error:', event);
                    this.updateStatus('Ready to listen');
                };

                this.synthesis.speak(utterance);
            }

            showLoading() {
                this.responseText.innerHTML = `
                    <div class="loading">
                        <div class="loading-dot"></div>
                        <div class="loading-dot"></div>
                        <div class="loading-dot"></div>
                    </div>
                    <div style="text-align: center; margin-top: 15px; color: #666;">
                        Getting AI response...
                    </div>
                `;
            }

            showError(message) {
                this.responseText.innerHTML = `
                    <div class="error-message">
                        <strong>‚ùå Error:</strong><br>
                        ${message}
                    </div>
                `;
            }

            clearResponse() {
                this.responseText.innerHTML = `
                    <div class="placeholder-text">
                        üéØ AI responses will appear here...<br><br>
                        üí° Try asking: "What's the weather like?" or "Tell me a joke!"
                    </div>
                `;
                this.synthesis.cancel();
                this.updateStatus('Ready to listen');
            }

            updateStatus(status) {
                this.statusText.textContent = status;
            }

            updateUI() {
                this.startBtn.disabled = this.isListening || this.isProcessing;
                this.stopBtn.disabled = !this.isListening;

                this.microphoneBtn.className = 'microphone';
                if (this.isListening) {
                    this.microphoneBtn.classList.add('listening');
                } else if (this.isProcessing) {
                    this.microphoneBtn.classList.add('processing');
                }

                if (this.isListening) {
                    this.microphoneBtn.textContent = 'üî¥';
                } else if (this.isProcessing) {
                    this.microphoneBtn.textContent = '‚öôÔ∏è';
                } else {
                    this.microphoneBtn.textContent = 'üé§';
                }
            }
        }

        document.addEventListener('DOMContentLoaded', () => {
            window.voiceAssistant = new VoiceAssistant();
        });
    </script>
</body>
</html>
*/
