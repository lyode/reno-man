<!DOCTYPE html>
<html lang="en">
<head>
// App.js - React Native with WebView
import React from 'react';
import { WebView } from 'react-native-webview';
import { StyleSheet, SafeAreaView, Platform } from 'react-native';

const VoiceAssistantHTML = `
<!-- YOUR COMPLETE HTML CODE GOES HERE -->
<!DOCTYPE html>
<html>
<!-- Paste the complete HTML code from the artifact here -->
</html>
`;

const App = () => {
  return (
    <SafeAreaView style={styles.container}>
      <WebView
        source={{ html: VoiceAssistantHTML }}
        style={styles.webview}
        javaScriptEnabled={true}
        domStorageEnabled={true}
        startInLoadingState={true}
        allowsInlineMediaPlayback={true}
        mediaPlaybackRequiresUserAction={false}
        originWhitelist={['*']}
        mixedContentMode="compatibility"
        onMessage={(event) => {
          console.log('Message from WebView:', event.nativeEvent.data);
        }}
        injectedJavaScript={`
          // Additional native integrations
          window.ReactNativeWebView.postMessage('WebView loaded');
          true;
        `}
      />
    </SafeAreaView>
  );
};

const styles = StyleSheet.create({
  container: {
    flex: 1,
    backgroundColor: '#000',
  },
  webview: {
    flex: 1,
  },
});

export default App;

// ==========================================
// 2. IONIC ANGULAR INTEGRATION
// ==========================================

// home.page.ts
import { Component } from '@angular/core';
import { SpeechRecognition } from '@ionic-native/speech-recognition/ngx';
import { TextToSpeech } from '@ionic-native/text-to-speech/ngx';

@Component({
  selector: 'app-home',
  templateUrl: 'home.page.html',
  styleUrls: ['home.page.scss'],
})
export class HomePage {
  isListening = false;
  responseText = '';
  statusText = 'Ready to listen';

  constructor(
    private speechRecognition: SpeechRecognition,
    private tts: TextToSpeech
  ) {}

  async startListening() {
    try {
      const hasPermission = await this.speechRecognition.hasPermission();
      if (!hasPermission) {
        await this.speechRecognition.requestPermission();
      }

      const options = {
        language: 'en-US',
        matches: 1,
        prompt: 'Speak now',
        showPopup: true,
        showPartial: false
      };

      this.isListening = true;
      this.statusText = 'Listening...';

      this.speechRecognition.startListening(options).subscribe(
        (matches: string[]) => {
          if (matches && matches.length > 0) {
            this.processUserInput(matches[0]);
          }
        },
        (onerror) => {
          console.error('Speech recognition error:', onerror);
          this.isListening = false;
          this.statusText = 'Error: ' + onerror;
        }
      );
    } catch (error) {
      console.error('Permission error:', error);
    }
  }

  stopListening() {
    this.speechRecognition.stopListening();
    this.isListening = false;
    this.statusText = 'Ready to listen';
  }

  async processUserInput(input: string) {
    this.statusText = 'Processing...';
    
    try {
      const response = await this.sendToChatGPT(input);
      this.responseText = response;
      await this.speakResponse(response);
    } catch (error) {
      this.responseText = 'Error: ' + error.message;
    }
    
    this.isListening = false;
    this.statusText = 'Ready to listen';
  }

  async sendToChatGPT(message: string): Promise<string> {
    const apiKey = 'sk-your-openai-api-key-here'; // Replace with your key
    const url = 'https://api.openai.com/v1/chat/completions';

    const response = await fetch(url, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${apiKey}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'gpt-3.5-turbo',
        messages: [
          {
            role: 'system',
            content: 'You are a helpful voice assistant. Provide concise responses.'
          },
          {
            role: 'user',
            content: message
          }
        ],
        max_tokens: 150
      })
    });

    const data = await response.json();
    return data.choices[0].message.content;
  }

  async speakResponse(text: string) {
    try {
      await this.tts.speak({
        text: text,
        locale: 'en-US',
        rate: 0.75
      });
    } catch (error) {
      console.error('TTS error:', error);
    }
  }
}

// home.page.html
/*
<ion-content [fullscreen]="true">
  <ion-header collapse="condense">
    <ion-toolbar>
      <ion-title size="large">AI Voice Assistant</ion-title>
    </ion-toolbar>
  </ion-header>

  <div class="container">
    <ion-card>
      <ion-card-header>
        <ion-card-title>{{ statusText }}</ion-card-title>
      </ion-card-header>
    </ion-card>

    <div class="microphone-container">
      <ion-button 
        [class]="isListening ? 'listening' : ''"
        shape="round" 
        size="large"
        (click)="isListening ? stopListening() : startListening()">
        <ion-icon name="mic" size="large"></ion-icon>
      </ion-button>
    </div>

    <div class="controls">
      <ion-button 
        expand="block" 
        [disabled]="isListening"
        (click)="startListening()">
        Start Listening
      </ion-button>
      
      <ion-button 
        expand="block" 
        color="danger"
        [disabled]="!isListening"
        (click)="stopListening()">
        Stop Listening
      </ion-button>
    </div>

    <ion-card>
      <ion-card-content>
        <p>{{ responseText || 'AI responses will appear here...' }}</p>
      </ion-card-content>
    </ion-card>
  </div>
</ion-content>
*/

// ==========================================
// 3. FLUTTER WEBVIEW INTEGRATION
// ==========================================

// main.dart
/*
import 'package:flutter/material.dart';
import 'package:webview_flutter/webview_flutter.dart';
import 'package:permission_handler/permission_handler.dart';

void main() {
  runApp(MyApp());
}

class MyApp extends StatelessWidget {
  @override
  Widget build(BuildContext context) {
    return MaterialApp(
      title: 'AI Voice Assistant',
      theme: ThemeData(primarySwatch: Colors.blue),
      home: VoiceAssistantPage(),
    );
  }
}

class VoiceAssistantPage extends StatefulWidget {
  @override
  _VoiceAssistantPageState createState() => _VoiceAssistantPageState();
}

class _VoiceAssistantPageState extends State<VoiceAssistantPage> {
  late WebViewController controller;

  @override
  void initState() {
    super.initState();
    requestPermissions();
  }

  Future<void> requestPermissions() async {
    await Permission.microphone.request();
    await Permission.speech.request();
  }

  @override
  Widget build(BuildContext context) {
    return Scaffold(
      appBar: AppBar(
        title: Text('AI Voice Assistant'),
        backgroundColor: Colors.blue,
      ),
      body: WebView(
        initialUrl: 'data:text/html;base64,' + base64Encode(
          const Utf8Encoder().convert(htmlContent)
        ),
        javascriptMode: JavascriptMode.unrestricted,
        onWebViewCreated: (WebViewController webViewController) {
          controller = webViewController;
        },
        navigationDelegate: (NavigationRequest request) {
          return NavigationDecision.navigate;
        },
      ),
    );
  }

  final String htmlContent = '''
    <!-- YOUR COMPLETE HTML CODE GOES HERE -->
  ''';
}
*/

// ==========================================
// 4. CORDOVA/PHONEGAP INTEGRATION
// ==========================================

// config.xml configuration
/*
<?xml version='1.0' encoding='utf-8'?>
<widget id="com.yourname.voiceassistant" version="1.0.0" xmlns="http://www.w3.org/ns/widgets" xmlns:gap="http://phonegap.com/ns/1.0">
    <name>AI Voice Assistant</name>
    <description>ChatGPT powered voice assistant</description>
    <author email="you@domain.com" href="http://yourwebsite.com">Your Name</author>

    <!-- Permissions -->
    <uses-permission android:name="android.permission.RECORD_AUDIO" />
    <uses-permission android:name="android.permission.INTERNET" />
    <uses-permission android:name="android.permission.ACCESS_NETWORK_STATE" />

    <!-- iOS Permissions -->
    <platform name="ios">
        <config-file target="*-Info.plist" parent="NSMicrophoneUsageDescription">
            <string>This app needs microphone access for voice commands</string>
        </config-file>
        <config-file target="*-Info.plist" parent="NSSpeechRecognitionUsageDescription">
            <string>This app needs speech recognition for voice commands</string>
        </config-file>
    </platform>

    <!-- Content Security Policy -->
    <meta http-equiv="Content-Security-Policy" content="default-src 'self' data: gap: https://ssl.gstatic.com 'unsafe-eval' 'unsafe-inline'; style-src 'self' 'unsafe-inline'; media-src *; img-src 'self' data: content:; connect-src * https://api.openai.com;">

    <!-- Plugins -->
    <plugin name="cordova-plugin-whitelist" version="1" />
    <plugin name="cordova-plugin-speechrecognition" version="1.2.0" />
    <plugin name="cordova-plugin-tts" version="0.2.3" />
    <plugin name="cordova-plugin-device" version="2.0.2" />
    <plugin name="cordova-plugin-splashscreen" version="5.0.2" />
</widget>
*/

// Enhanced JavaScript for Cordova
document.addEventListener('deviceready', function() {
    console.log('Cordova device ready');
    
    // Enhanced VoiceAssistant class for Cordova
    class CordovaVoiceAssistant extends VoiceAssistant {
        constructor() {
            super();
            this.setupCordovaFeatures();
        }

        setupCordovaFeatures() {
            // Use Cordova speech recognition plugin if available
            if (window.plugins && window.plugins.speechRecognition) {
                this.useCordovaSpeech = true;
                console.log('Using Cordova speech recognition');
            }

            // Handle back button
            document.addEventListener('backbutton', (e) => {
                e.preventDefault();
                if (this.isListening) {
                    this.stopListening();
                } else {
                    navigator.app.exitApp();
                }
            });

            // Handle pause/resume
            document.addEventListener('pause', () => {
                if (this.isListening) {
                    this.stopListening();
                }
            });
        }

        startListening() {
            if (this.useCordovaSpeech && window.plugins.speechRecognition) {
                this.startCordovaSpeechRecognition();
            } else {
                super.startListening();
            }
        }

        startCordovaSpeechRecognition() {
            const options = {
                language: 'en-US',
                matches: 1,
                prompt: 'Speak now',
                showPopup: true,
                showPartial: false
            };

            window.plugins.speechRecognition.startListening(
                (result) => {
                    if (result && result.length > 0) {
                        this.processUserInput(result[0]);
                    }
                },
                (error) => {
                    console.error('Speech recognition error:', error);
                    this.showError('Speech recognition error: ' + error);
                },
                options
            );

            this.isListening = true;
            this.updateUI();
        }

        speakResponse(text) {
            // Use Cordova TTS if available
            if (window.plugins && window.plugins.tts) {
                window.plugins.tts.speak({
                    text: text,
                    locale: 'en-US',
                    rate: 0.75
                }, () => {
                    this.updateStatus('✅ Ready to listen');
                }, (error) => {
                    console.error('TTS error:', error);
                    super.speakResponse(text);
                });
            } else {
                super.speakResponse(text);
            }
        }
    }

    // Initialize Cordova version
    window.voiceAssistant = new CordovaVoiceAssistant();
});

// ==========================================
// 5. PWA SERVICE WORKER
// ==========================================

// sw.js - Service Worker for PWA
const CACHE_NAME = 'voice-assistant-v1';
const urlsToCache = [
    '/',
    '/index.html',
    '/manifest.json'
];

self.addEventListener('install', (event) => {
    event.waitUntil(
        caches.open(CACHE_NAME)
            .then((cache) => cache.addAll(urlsToCache))
    );
});

self.addEventListener('fetch', (event) => {
    event.respondWith(
        caches.match(event.request)
            .then((response) => {
                // Return cached version or fetch from network
                return response || fetch(event.request);
            })
    );
});

// ==========================================
// 6. EXPO/REACT NATIVE EXPO INTEGRATION
// ==========================================

// App.js for Expo
/*
import React from 'react';
import { WebView } from 'react-native-webview';
import { StyleSheet, SafeAreaView } from 'react-native';
import * as Permissions from 'expo-permissions';
import { Audio } from 'expo-av';

export default class App extends React.Component {
    componentDidMount() {
        this.requestPermissions();
    }

    async requestPermissions() {
        const { status } = await Permissions.askAsync(Permissions.AUDIO_RECORDING);
        if (status !== 'granted') {
            alert('Permission to access microphone is required!');
        }

        await Audio.requestPermissionsAsync();
    }

    render() {
        return (
            <SafeAreaView style={styles.container}>
                <WebView
                    source={{ html: htmlContent }}
                    style={styles.webview}
                    javaScriptEnabled={true}
                    domStorageEnabled={true}
                    allowsInlineMediaPlayback={true}
                    mediaPlaybackRequiresUserAction={false}
                />
            </SafeAreaView>
        );
    }
}

const htmlContent = `
    <!-- YOUR COMPLETE HTML CODE GOES HERE -->
`;

const styles = StyleSheet.create({
    container: {
        flex: 1,
        backgroundColor: '#000',
    },
    webview: {
        flex: 1,
    },
});
*/

// ==========================================
// 7. DEPLOYMENT SCRIPTS
// ==========================================

// package.json scripts for different platforms
/*
{
  "scripts": {
    "build-cordova-ios": "cordova build ios --release",
    "build-cordova-android": "cordova build android --release",
    "build-ionic-ios": "ionic capacitor build ios",
    "build-ionic-android": "ionic capacitor build android",
    "build-react-native-ios": "react-native run-ios --configuration Release",
    "build-react-native-android": "react-native run-android --variant=release",
    "serve-pwa": "python -m http.server 8000",
    "deploy-pwa": "rsync -avz --delete ./ user@server:/var/www/voice-assistant/"
  }
}
*/

// ==========================================
// 8. ENVIRONMENT CONFIGURATION
// ==========================================

// .env file for different environments
/*
# Development
OPENAI_API_KEY=sk-dev-key-here
API_ENDPOINT=https://api.openai.com/v1/chat/completions
DEBUG=true

# Production  
OPENAI_API_KEY=sk-prod-key-here
API_ENDPOINT=https://your-proxy-server.com/api/chat
DEBUG=false

# Staging
OPENAI_API_KEY=sk-staging-key-here
API_ENDPOINT=https://staging-api.yourserver.com/chat
DEBUG=true
*/

// Environment handler
class EnvironmentConfig {
    constructor() {
        this.isDevelopment = window.location.hostname === 'localhost';
        this.apiKey = this.isDevelopment ? 
            'sk-dev-key-here' : 
            'sk-prod-key-here';
        this.apiEndpoint = this.isDevelopment ?
            'https://api.openai.com/v1/chat/completions' :
            '/api/chat'; // Proxy endpoint
    }

    getConfig() {
        return {
            apiKey: this.apiKey,
            apiEndpoint: this.apiEndpoint,
            debug: this.isDevelopment
        };
    }
}            }
        }
    </script>
</body>
</html>
